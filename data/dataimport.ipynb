{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify 'ecogridaidata' s3 bucket is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 10:15:58 ecogridaidata\n",
      "2025-03-03 19:04:48 sagemaker-studio-wmhb0pxgu8\n",
      "2025-03-03 19:04:50 sagemaker-us-east-1-571350132829\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 ls s3://${bucket}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'J3YSSZJNCW1CDCBK', 'HostId': 'o+tQzMk17wQJOt4DKf15OoFXC3CN4AoyZeSLlDd9bGs7HinATaFqqiaNSqJl5k4t6mlRyvCPib0=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'o+tQzMk17wQJOt4DKf15OoFXC3CN4AoyZeSLlDd9bGs7HinATaFqqiaNSqJl5k4t6mlRyvCPib0=', 'x-amz-request-id': 'J3YSSZJNCW1CDCBK', 'date': 'Sun, 23 Mar 2025 13:45:51 GMT', 'x-amz-bucket-region': 'us-east-1', 'x-amz-access-point-alias': 'false', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'BucketRegion': 'us-east-1', 'AccessPointAlias': False}\n"
     ]
    }
   ],
   "source": [
    "from botocore.client import ClientError\n",
    "\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    response = s3.head_bucket(Bucket='ecogridaidata')\n",
    "    print(response)\n",
    "    setup_s3_bucket_passed = True\n",
    "except ClientError as e:\n",
    "    print(\"[ERROR] Cannot find bucket {} in {} due to {}.\".format(bucket, response, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'setup_s3_bucket_passed' (bool)\n"
     ]
    }
   ],
   "source": [
    "%store setup_s3_bucket_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "setup_dependencies_passed               -> True\n",
      "setup_iam_roles_passed                  -> True\n",
      "setup_instance_check_passed             -> True\n",
      "setup_s3_bucket_passed                  -> True\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store data into 'ecogridaidata' s3 folder through API integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching records 0 to 5000...\n",
      "Fetching records 5000 to 10000...\n",
      "Fetching records 10000 to 15000...\n",
      "Fetching records 15000 to 20000...\n",
      "Fetching records 20000 to 25000...\n",
      "Fetching records 25000 to 30000...\n",
      "Fetching records 30000 to 35000...\n",
      "Fetching records 35000 to 40000...\n",
      "Fetching records 40000 to 45000...\n",
      "Fetching records 45000 to 50000...\n",
      "Fetching records 50000 to 55000...\n",
      "Fetching records 55000 to 60000...\n",
      "Fetching records 60000 to 65000...\n",
      "Fetching records 65000 to 70000...\n",
      "Fetching records 70000 to 75000...\n",
      "Fetching records 75000 to 80000...\n",
      "Fetching records 80000 to 85000...\n",
      "Fetching records 85000 to 90000...\n",
      "Fetching records 90000 to 95000...\n",
      "Fetching records 95000 to 100000...\n",
      "Fetching records 100000 to 105000...\n",
      "Fetching records 105000 to 110000...\n",
      "Fetching records 110000 to 115000...\n",
      "Fetching records 115000 to 120000...\n",
      "Fetching records 120000 to 125000...\n",
      "Fetching records 125000 to 130000...\n",
      "Fetching records 130000 to 135000...\n",
      "Fetching records 135000 to 140000...\n",
      "Fetching records 140000 to 145000...\n",
      "Fetching records 145000 to 150000...\n",
      "Fetching records 150000 to 155000...\n",
      "Fetching records 155000 to 160000...\n",
      "Fetching records 160000 to 165000...\n",
      "Fetching records 165000 to 170000...\n",
      "Fetching records 170000 to 175000...\n",
      "Fetching records 175000 to 180000...\n",
      "Fetching records 180000 to 185000...\n",
      "Fetching records 185000 to 190000...\n",
      "Fetching records 190000 to 195000...\n",
      "Fetching records 195000 to 200000...\n",
      "Fetching records 200000 to 205000...\n",
      "Fetching records 205000 to 210000...\n",
      "Fetching records 210000 to 215000...\n",
      "Fetching records 215000 to 220000...\n",
      "Fetching records 220000 to 225000...\n",
      "Fetching records 225000 to 230000...\n",
      "Fetching records 230000 to 235000...\n",
      "Fetching records 235000 to 240000...\n",
      "Fetching records 240000 to 245000...\n",
      "Fetching records 245000 to 250000...\n",
      "Fetching records 250000 to 255000...\n",
      "Fetching records 255000 to 260000...\n",
      "Fetching records 260000 to 265000...\n",
      "Fetching records 265000 to 270000...\n",
      "Fetching records 270000 to 275000...\n",
      "Fetching records 275000 to 280000...\n",
      "Fetching records 280000 to 285000...\n",
      "Fetching records 285000 to 290000...\n",
      "Fetching records 290000 to 295000...\n",
      "Fetching records 295000 to 300000...\n",
      "Fetching records 300000 to 305000...\n",
      "Fetching records 305000 to 310000...\n",
      "Fetching records 310000 to 315000...\n",
      "Fetching records 315000 to 320000...\n",
      "Fetching records 320000 to 325000...\n",
      "Fetching records 325000 to 330000...\n",
      "Fetching records 330000 to 335000...\n",
      "Fetching records 335000 to 340000...\n",
      "Fetching records 340000 to 345000...\n",
      "Fetching records 345000 to 350000...\n",
      "Fetching records 350000 to 355000...\n",
      "Fetching records 355000 to 360000...\n",
      "Fetching records 360000 to 365000...\n",
      "Fetching records 365000 to 370000...\n",
      "Fetching records 370000 to 375000...\n",
      "Fetching records 375000 to 380000...\n",
      "Fetching records 380000 to 385000...\n",
      "Fetching records 385000 to 390000...\n",
      "Fetching records 390000 to 395000...\n",
      "Fetching records 395000 to 400000...\n",
      "Fetching records 400000 to 405000...\n",
      "Fetching records 405000 to 410000...\n",
      "Fetching records 410000 to 415000...\n",
      "Fetching records 415000 to 420000...\n",
      "Fetching records 420000 to 425000...\n",
      "Fetching records 425000 to 430000...\n",
      "Fetching records 430000 to 435000...\n",
      "Fetching records 435000 to 440000...\n",
      "Fetching records 440000 to 445000...\n",
      "Fetching records 445000 to 450000...\n",
      "Fetching records 450000 to 455000...\n",
      "Fetching records 455000 to 460000...\n",
      "Fetching records 460000 to 465000...\n",
      "Fetching records 465000 to 470000...\n",
      "Fetching records 470000 to 475000...\n",
      "Fetching records 475000 to 480000...\n",
      "Fetching records 480000 to 485000...\n",
      "Fetching records 485000 to 490000...\n",
      "Fetching records 490000 to 495000...\n",
      "Fetching records 495000 to 500000...\n",
      "Fetching records 500000 to 505000...\n",
      "Fetching records 505000 to 510000...\n",
      "Fetching records 510000 to 515000...\n",
      "Fetching records 515000 to 520000...\n",
      "Fetching records 520000 to 525000...\n",
      "Fetching records 525000 to 530000...\n",
      "Fetching records 530000 to 535000...\n",
      "Fetching records 535000 to 540000...\n",
      "Fetching records 540000 to 545000...\n",
      "Fetching records 545000 to 550000...\n",
      "Fetching records 550000 to 555000...\n",
      "Fetching records 555000 to 560000...\n",
      "Fetching records 560000 to 565000...\n",
      "Fetching records 565000 to 570000...\n",
      "Fetching records 570000 to 575000...\n",
      "Fetching records 575000 to 580000...\n",
      "Fetching records 580000 to 585000...\n",
      "Fetching records 585000 to 590000...\n",
      "Fetching records 590000 to 595000...\n",
      "Fetching records 595000 to 600000...\n",
      "Fetching records 600000 to 605000...\n",
      "Fetching records 605000 to 610000...\n",
      "Fetching records 610000 to 615000...\n",
      "Fetching records 615000 to 620000...\n",
      "Fetching records 620000 to 625000...\n",
      "Fetching records 625000 to 630000...\n",
      "Fetching records 630000 to 635000...\n",
      "Fetching records 635000 to 640000...\n",
      "Fetching records 640000 to 645000...\n",
      "Fetching records 645000 to 650000...\n",
      "Fetching records 650000 to 655000...\n",
      "Fetching records 655000 to 660000...\n",
      "Fetching records 660000 to 665000...\n",
      "Fetching records 665000 to 670000...\n",
      "Fetching records 670000 to 675000...\n",
      "Fetching records 675000 to 680000...\n",
      "Fetching records 680000 to 685000...\n",
      "Fetching records 685000 to 690000...\n",
      "Fetching records 690000 to 695000...\n",
      "Fetching records 695000 to 700000...\n",
      "Fetching records 700000 to 705000...\n",
      "Fetching records 705000 to 710000...\n",
      "Fetching records 710000 to 715000...\n",
      "Fetching records 715000 to 720000...\n",
      "Fetching records 720000 to 725000...\n",
      "Fetching records 725000 to 730000...\n",
      "Fetching records 730000 to 735000...\n",
      "Fetching records 735000 to 740000...\n",
      "Fetching records 740000 to 745000...\n",
      "Fetching records 745000 to 750000...\n",
      "Fetching records 750000 to 755000...\n",
      "Fetching records 755000 to 760000...\n",
      "Fetching records 760000 to 765000...\n",
      "Fetching records 765000 to 770000...\n",
      "Fetching records 770000 to 775000...\n",
      "Fetching records 775000 to 780000...\n",
      "Fetching records 780000 to 785000...\n",
      "Fetching records 785000 to 790000...\n",
      "Fetching records 790000 to 795000...\n",
      "Fetching records 795000 to 800000...\n",
      "Fetching records 800000 to 805000...\n",
      "Fetching records 805000 to 810000...\n",
      "Fetching records 810000 to 815000...\n",
      "Fetching records 815000 to 820000...\n",
      "Fetching records 820000 to 825000...\n",
      "Fetching records 825000 to 830000...\n",
      "Fetching records 830000 to 835000...\n",
      "Fetching records 835000 to 840000...\n",
      "Fetching records 840000 to 845000...\n",
      "Fetching records 845000 to 850000...\n",
      "Fetching records 850000 to 855000...\n",
      "Fetching records 855000 to 860000...\n",
      "Fetching records 860000 to 865000...\n",
      "Fetching records 865000 to 870000...\n",
      "Fetching records 870000 to 875000...\n",
      "Fetching records 875000 to 880000...\n",
      "Fetching records 880000 to 885000...\n",
      "Fetching records 885000 to 890000...\n",
      "Fetching records 890000 to 895000...\n",
      "Fetching records 895000 to 900000...\n",
      "Fetching records 900000 to 905000...\n",
      "Fetching records 905000 to 910000...\n",
      "Fetching records 910000 to 915000...\n",
      "Fetching records 915000 to 920000...\n",
      "Fetching records 920000 to 925000...\n",
      "Fetching records 925000 to 930000...\n",
      "Fetching records 930000 to 935000...\n",
      "Fetching records 935000 to 940000...\n",
      "Fetching records 940000 to 945000...\n",
      "Fetching records 945000 to 950000...\n",
      "Fetching records 950000 to 955000...\n",
      "Fetching records 955000 to 960000...\n",
      "Fetching records 960000 to 965000...\n",
      "Fetching records 965000 to 970000...\n",
      "Fetching records 970000 to 975000...\n",
      "Fetching records 975000 to 980000...\n",
      "Fetching records 980000 to 985000...\n",
      "Fetching records 985000 to 990000...\n",
      "Fetching records 990000 to 995000...\n",
      "Fetching records 995000 to 1000000...\n",
      "Fetching records 1000000 to 1005000...\n",
      "Fetching records 1005000 to 1010000...\n",
      "Fetching records 1010000 to 1015000...\n",
      "Fetching records 1015000 to 1020000...\n",
      "Fetching records 1020000 to 1025000...\n",
      "Fetching records 1025000 to 1030000...\n",
      "Fetching records 1030000 to 1035000...\n",
      "Fetching records 1035000 to 1040000...\n",
      "Trimmed to exact target record count.\n",
      "✅ Total records fetched: 1038828\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/eia_demandforecast.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(all_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Upload to S3\u001b[39;00m\n\u001b[1;32m     70\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "api_key = \"g8IFZafpLwkh0hRmYC8xE91RQ1qa0Rggf567XdZe\"\n",
    "base_url = \"https://api.eia.gov/v2/electricity/rto/daily-region-data/data/\"\n",
    "bucket_name = \"ecogridaidata\"\n",
    "s3_folder = \"eia_electricity\"\n",
    "target_total = 1_038_828\n",
    "\n",
    "# === API PARAMS TEMPLATE ===\n",
    "params_template = {\n",
    "    \"frequency\": \"daily\",\n",
    "    \"data\": [\"value\"],\n",
    "    \"facets\": {},\n",
    "    \"start\": \"2024-01-01\",\n",
    "    \"end\": \"2024-12-31\",\n",
    "    \"sort\": [{\"column\": \"period\", \"direction\": \"desc\"}],\n",
    "    \"offset\": 0,\n",
    "    \"length\": 5000\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === DATA COLLECTION LOOP ===\n",
    "all_data = []\n",
    "offset = 0\n",
    "length = params_template[\"length\"]\n",
    "\n",
    "while len(all_data) < target_total:\n",
    "    print(f\"Fetching records {offset} to {offset + length}...\")\n",
    "\n",
    "    params = params_template.copy()\n",
    "    params[\"offset\"] = offset\n",
    "\n",
    "    response = requests.get(\n",
    "        base_url,\n",
    "        headers=headers,\n",
    "        params={\"api_key\": api_key},\n",
    "        json=params\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error at offset {offset}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    response_json = response.json()\n",
    "    records = response_json.get(\"response\", {}).get(\"data\", [])\n",
    "\n",
    "    if not records:\n",
    "        print(\"No more data returned by API.\")\n",
    "        break\n",
    "\n",
    "    all_data.extend(records)\n",
    "    offset += length\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Trim if we fetched slightly over\n",
    "    if len(all_data) > target_total:\n",
    "        all_data = all_data[:target_total]\n",
    "        print(\"Trimmed to exact target record count.\")\n",
    "\n",
    "print(f\"✅ Total records fetched: {len(all_data)}\")\n",
    "\n",
    "# === SAVE TO JSON ===\n",
    "json_path = \"/tmp/eia_demandforecast.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "s3_key = f\"{s3_folder}/eia_data_demand_{timestamp}.json\"\n",
    "\n",
    "s3.upload_file(json_path, bucket_name, s3_key)\n",
    "print(f\"✅ Uploaded to s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching records 0 to 5000...\n",
      "🔍 Sample keys in record: dict_keys(['period', 'subba', 'subba-name', 'parent', 'parent-name', 'timezone', 'value', 'value-units'])\n",
      "📦 Sample record preview:\n",
      " {\n",
      "  \"period\": \"2024-12-31\",\n",
      "  \"subba\": \"PGAE\",\n",
      "  \"subba-name\": \"Pacific Gas and Electric\",\n",
      "  \"parent\": \"CISO\",\n",
      "  \"parent-name\": \"California Independent System Operator\",\n",
      "  \"timezone\": \"Arizona\",\n",
      "  \"value\": \"247213\",\n",
      "  \"value-units\": \"megawatthours\"\n",
      "}\n",
      "Fetching records 5000 to 10000...\n",
      "Fetching records 10000 to 15000...\n",
      "Fetching records 15000 to 20000...\n",
      "Fetching records 20000 to 25000...\n",
      "Fetching records 25000 to 30000...\n",
      "Fetching records 30000 to 35000...\n",
      "Fetching records 35000 to 40000...\n",
      "Fetching records 40000 to 45000...\n",
      "Fetching records 45000 to 50000...\n",
      "Fetching records 50000 to 55000...\n",
      "Fetching records 55000 to 60000...\n",
      "Fetching records 60000 to 65000...\n",
      "Fetching records 65000 to 70000...\n",
      "Fetching records 70000 to 75000...\n",
      "Fetching records 75000 to 80000...\n",
      "Fetching records 80000 to 85000...\n",
      "Fetching records 85000 to 90000...\n",
      "Fetching records 90000 to 95000...\n",
      "Fetching records 95000 to 100000...\n",
      "Fetching records 100000 to 105000...\n",
      "Fetching records 105000 to 110000...\n",
      "Fetching records 110000 to 115000...\n",
      "Fetching records 115000 to 120000...\n",
      "Fetching records 120000 to 125000...\n",
      "Fetching records 125000 to 130000...\n",
      "Fetching records 130000 to 135000...\n",
      "Fetching records 135000 to 140000...\n",
      "Fetching records 140000 to 145000...\n",
      "Fetching records 145000 to 150000...\n",
      "Fetching records 150000 to 155000...\n",
      "Fetching records 155000 to 160000...\n",
      "Fetching records 160000 to 165000...\n",
      "Fetching records 165000 to 170000...\n",
      "Fetching records 170000 to 175000...\n",
      "Fetching records 175000 to 180000...\n",
      "Fetching records 180000 to 185000...\n",
      "Fetching records 185000 to 190000...\n",
      "Fetching records 190000 to 195000...\n",
      "Fetching records 195000 to 200000...\n",
      "Fetching records 200000 to 205000...\n",
      "Fetching records 205000 to 210000...\n",
      "Fetching records 210000 to 215000...\n",
      "Fetching records 215000 to 220000...\n",
      "Fetching records 220000 to 225000...\n",
      "Fetching records 225000 to 230000...\n",
      "Fetching records 230000 to 235000...\n",
      "Fetching records 235000 to 240000...\n",
      "Fetching records 240000 to 245000...\n",
      "Fetching records 245000 to 250000...\n",
      "Fetching records 250000 to 255000...\n",
      "Fetching records 255000 to 260000...\n",
      "Fetching records 260000 to 265000...\n",
      "Fetching records 265000 to 270000...\n",
      "Fetching records 270000 to 275000...\n",
      "Fetching records 275000 to 280000...\n",
      "Fetching records 280000 to 285000...\n",
      "Fetching records 285000 to 290000...\n",
      "Fetching records 290000 to 295000...\n",
      "Fetching records 295000 to 300000...\n",
      "Fetching records 300000 to 305000...\n",
      "✅ Total records fetched: 302719\n",
      "📄 Flattened CSV saved to /tmp/eia_demandforecast_subregion_flat.csv\n",
      "🧾 Flattened columns: ['period', 'subba', 'subba-name', 'parent', 'parent-name', 'timezone', 'value', 'value-units']\n",
      "✅ Uploaded raw JSON to s3://ecogridaidata/eia_electricity/eia_data_subregion_20250323_191441.json\n"
     ]
    }
   ],
   "source": [
    "# API for subregion data, used chatgpt on 3/23/2025 to assist with API connection of data.\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "api_key = \"g8IFZafpLwkh0hRmYC8xE91RQ1qa0Rggf567XdZe\"\n",
    "base_url = \"https://api.eia.gov/v2/electricity/rto/daily-region-sub-ba-data/data/\"\n",
    "bucket_name = \"ecogridaidata\"\n",
    "s3_folder = \"eia_electricity\"\n",
    "target_total = 302_719\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === STATIC PARAMS TEMPLATE (no offset/length here) ===\n",
    "params_template = {\n",
    "    \"api_key\": api_key,\n",
    "    \"frequency\": \"daily\",\n",
    "    \"start\": \"2023-01-01\",\n",
    "    \"end\": \"2024-12-31\",\n",
    "    \"data[0]\": \"value\",\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"desc\"\n",
    "}\n",
    "\n",
    "# === DATA COLLECTION LOOP ===\n",
    "all_data = []\n",
    "offset = 0\n",
    "length = 5000\n",
    "\n",
    "while len(all_data) < target_total:\n",
    "    print(f\"Fetching records {offset} to {offset + length}...\")\n",
    "\n",
    "    # Combine static params with dynamic offset and length\n",
    "    params = params_template.copy()\n",
    "    params[\"offset\"] = offset\n",
    "    params[\"length\"] = length\n",
    "\n",
    "    response = requests.get(\n",
    "        base_url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error at offset {offset}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    response_json = response.json()\n",
    "    records = response_json.get(\"response\", {}).get(\"data\", [])\n",
    "\n",
    "    if not records:\n",
    "        print(\"No more data returned by API.\")\n",
    "        break\n",
    "\n",
    "    # 👀 Inspect sample record on first batch\n",
    "    if offset == 0 and records:\n",
    "        print(\"🔍 Sample keys in record:\", records[0].keys())\n",
    "        print(\"📦 Sample record preview:\\n\", json.dumps(records[0], indent=2))\n",
    "\n",
    "    all_data.extend(records)\n",
    "    offset += length\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Trim if slightly over\n",
    "    if len(all_data) > target_total:\n",
    "        all_data = all_data[:target_total]\n",
    "        print(\"Trimmed to exact target record count.\")\n",
    "\n",
    "print(f\"✅ Total records fetched: {len(all_data)}\")\n",
    "\n",
    "# === SAVE RAW JSON ===\n",
    "json_path = \"/tmp/eia_demandforecast_subregion.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "\n",
    "# === FLATTEN AND SAVE PREVIEW CSV ===\n",
    "df = json_normalize(all_data)\n",
    "df.columns = [col.replace(\".\", \"_\") for col in df.columns]  # ✅ Safe column name cleanup\n",
    "\n",
    "csv_path = \"/tmp/eia_demandforecast_subregion_flat.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"📄 Flattened CSV saved to {csv_path}\")\n",
    "print(\"🧾 Flattened columns:\", df.columns.tolist())\n",
    "\n",
    "# === UPLOAD TO S3 (JSON file) ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "s3_key = f\"{s3_folder}/eia_data_subregion_{timestamp}.json\"\n",
    "\n",
    "s3.upload_file(json_path, bucket_name, s3_key)\n",
    "print(f\"✅ Uploaded raw JSON to s3://{bucket_name}/{s3_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching records 0 to 5000...\n",
      "🔍 Sample keys in record: dict_keys(['period', 'respondent', 'respondent-name', 'fueltype', 'type-name', 'timezone', 'timezone-description', 'value', 'value-units'])\n",
      "📦 Sample record preview:\n",
      " {\n",
      "  \"period\": \"2024-12-31\",\n",
      "  \"respondent\": \"AECI\",\n",
      "  \"respondent-name\": \"Associated Electric Cooperative, Inc.\",\n",
      "  \"fueltype\": \"COL\",\n",
      "  \"type-name\": \"Coal\",\n",
      "  \"timezone\": \"Arizona\",\n",
      "  \"timezone-description\": \"Arizona\",\n",
      "  \"value\": \"23782\",\n",
      "  \"value-units\": \"megawatthours\"\n",
      "}\n",
      "Fetching records 5000 to 10000...\n",
      "Fetching records 10000 to 15000...\n",
      "Fetching records 15000 to 20000...\n",
      "Fetching records 20000 to 25000...\n",
      "Fetching records 25000 to 30000...\n",
      "Fetching records 30000 to 35000...\n",
      "Fetching records 35000 to 40000...\n",
      "Fetching records 40000 to 45000...\n",
      "Fetching records 45000 to 50000...\n",
      "Fetching records 50000 to 55000...\n",
      "Fetching records 55000 to 60000...\n",
      "Fetching records 60000 to 65000...\n",
      "Fetching records 65000 to 70000...\n",
      "Fetching records 70000 to 75000...\n",
      "Fetching records 75000 to 80000...\n",
      "Fetching records 80000 to 85000...\n",
      "Fetching records 85000 to 90000...\n",
      "Fetching records 90000 to 95000...\n",
      "Fetching records 95000 to 100000...\n",
      "Fetching records 100000 to 105000...\n",
      "Fetching records 105000 to 110000...\n",
      "Fetching records 110000 to 115000...\n",
      "Fetching records 115000 to 120000...\n",
      "Fetching records 120000 to 125000...\n",
      "Fetching records 125000 to 130000...\n",
      "Fetching records 130000 to 135000...\n",
      "Fetching records 135000 to 140000...\n",
      "Fetching records 140000 to 145000...\n",
      "Fetching records 145000 to 150000...\n",
      "Fetching records 150000 to 155000...\n",
      "Fetching records 155000 to 160000...\n",
      "Fetching records 160000 to 165000...\n",
      "Fetching records 165000 to 170000...\n",
      "Fetching records 170000 to 175000...\n",
      "Fetching records 175000 to 180000...\n",
      "Fetching records 180000 to 185000...\n",
      "Fetching records 185000 to 190000...\n",
      "Fetching records 190000 to 195000...\n",
      "Fetching records 195000 to 200000...\n",
      "Fetching records 200000 to 205000...\n",
      "Fetching records 205000 to 210000...\n",
      "Fetching records 210000 to 215000...\n",
      "Fetching records 215000 to 220000...\n",
      "Fetching records 220000 to 225000...\n",
      "Fetching records 225000 to 230000...\n",
      "Fetching records 230000 to 235000...\n",
      "Fetching records 235000 to 240000...\n",
      "Fetching records 240000 to 245000...\n",
      "Fetching records 245000 to 250000...\n",
      "Fetching records 250000 to 255000...\n",
      "Fetching records 255000 to 260000...\n",
      "Fetching records 260000 to 265000...\n",
      "Fetching records 265000 to 270000...\n",
      "Fetching records 270000 to 275000...\n",
      "Fetching records 275000 to 280000...\n",
      "Fetching records 280000 to 285000...\n",
      "Fetching records 285000 to 290000...\n",
      "Fetching records 290000 to 295000...\n",
      "Fetching records 295000 to 300000...\n",
      "Fetching records 300000 to 305000...\n",
      "Fetching records 305000 to 310000...\n",
      "Fetching records 310000 to 315000...\n",
      "Fetching records 315000 to 320000...\n",
      "Fetching records 320000 to 325000...\n",
      "Fetching records 325000 to 330000...\n",
      "Fetching records 330000 to 335000...\n",
      "Fetching records 335000 to 340000...\n",
      "Fetching records 340000 to 345000...\n",
      "Fetching records 345000 to 350000...\n",
      "Fetching records 350000 to 355000...\n",
      "Fetching records 355000 to 360000...\n",
      "Fetching records 360000 to 365000...\n",
      "Fetching records 365000 to 370000...\n",
      "Fetching records 370000 to 375000...\n",
      "Fetching records 375000 to 380000...\n",
      "Fetching records 380000 to 385000...\n",
      "Fetching records 385000 to 390000...\n",
      "Fetching records 390000 to 395000...\n",
      "Fetching records 395000 to 400000...\n",
      "Fetching records 400000 to 405000...\n",
      "Fetching records 405000 to 410000...\n",
      "Fetching records 410000 to 415000...\n",
      "Fetching records 415000 to 420000...\n",
      "Fetching records 420000 to 425000...\n",
      "Fetching records 425000 to 430000...\n",
      "Fetching records 430000 to 435000...\n",
      "Fetching records 435000 to 440000...\n",
      "Fetching records 440000 to 445000...\n",
      "Fetching records 445000 to 450000...\n",
      "Fetching records 450000 to 455000...\n",
      "Fetching records 455000 to 460000...\n",
      "Fetching records 460000 to 465000...\n",
      "Fetching records 465000 to 470000...\n",
      "Fetching records 470000 to 475000...\n",
      "Fetching records 475000 to 480000...\n",
      "Fetching records 480000 to 485000...\n",
      "Fetching records 485000 to 490000...\n",
      "Fetching records 490000 to 495000...\n",
      "Fetching records 495000 to 500000...\n",
      "Fetching records 500000 to 505000...\n",
      "Fetching records 505000 to 510000...\n",
      "Fetching records 510000 to 515000...\n",
      "Fetching records 515000 to 520000...\n",
      "Fetching records 520000 to 525000...\n",
      "Fetching records 525000 to 530000...\n",
      "Fetching records 530000 to 535000...\n",
      "Fetching records 535000 to 540000...\n",
      "Fetching records 540000 to 545000...\n",
      "Fetching records 545000 to 550000...\n",
      "Fetching records 550000 to 555000...\n",
      "Fetching records 555000 to 560000...\n",
      "Fetching records 560000 to 565000...\n",
      "Fetching records 565000 to 570000...\n",
      "Fetching records 570000 to 575000...\n",
      "Fetching records 575000 to 580000...\n",
      "Fetching records 580000 to 585000...\n",
      "Fetching records 585000 to 590000...\n",
      "Fetching records 590000 to 595000...\n",
      "Fetching records 595000 to 600000...\n",
      "Fetching records 600000 to 605000...\n",
      "Fetching records 605000 to 610000...\n",
      "Fetching records 610000 to 615000...\n",
      "Fetching records 615000 to 620000...\n",
      "Fetching records 620000 to 625000...\n",
      "Fetching records 625000 to 630000...\n",
      "Fetching records 630000 to 635000...\n",
      "Fetching records 635000 to 640000...\n",
      "Fetching records 640000 to 645000...\n",
      "Fetching records 645000 to 650000...\n",
      "Fetching records 650000 to 655000...\n",
      "Fetching records 655000 to 660000...\n",
      "Fetching records 660000 to 665000...\n",
      "Fetching records 665000 to 670000...\n",
      "Fetching records 670000 to 675000...\n",
      "Fetching records 675000 to 680000...\n",
      "Fetching records 680000 to 685000...\n",
      "Fetching records 685000 to 690000...\n",
      "Fetching records 690000 to 695000...\n",
      "Fetching records 695000 to 700000...\n",
      "Fetching records 700000 to 705000...\n",
      "Fetching records 705000 to 710000...\n",
      "Fetching records 710000 to 715000...\n",
      "Fetching records 715000 to 720000...\n",
      "Fetching records 720000 to 725000...\n",
      "Fetching records 725000 to 730000...\n",
      "Fetching records 730000 to 735000...\n",
      "Fetching records 735000 to 740000...\n",
      "Fetching records 740000 to 745000...\n",
      "Fetching records 745000 to 750000...\n",
      "Fetching records 750000 to 755000...\n",
      "Fetching records 755000 to 760000...\n",
      "Fetching records 760000 to 765000...\n",
      "Fetching records 765000 to 770000...\n",
      "Fetching records 770000 to 775000...\n",
      "Fetching records 775000 to 780000...\n",
      "Fetching records 780000 to 785000...\n",
      "Fetching records 785000 to 790000...\n",
      "Fetching records 790000 to 795000...\n",
      "Fetching records 795000 to 800000...\n",
      "Fetching records 800000 to 805000...\n",
      "Fetching records 805000 to 810000...\n",
      "Fetching records 810000 to 815000...\n",
      "Fetching records 815000 to 820000...\n",
      "Fetching records 820000 to 825000...\n",
      "Fetching records 825000 to 830000...\n",
      "Fetching records 830000 to 835000...\n",
      "Fetching records 835000 to 840000...\n",
      "Fetching records 840000 to 845000...\n",
      "Fetching records 845000 to 850000...\n",
      "Fetching records 850000 to 855000...\n",
      "Fetching records 855000 to 860000...\n",
      "Fetching records 860000 to 865000...\n",
      "Fetching records 865000 to 870000...\n",
      "Fetching records 870000 to 875000...\n",
      "Fetching records 875000 to 880000...\n",
      "Fetching records 880000 to 885000...\n",
      "Fetching records 885000 to 890000...\n",
      "Fetching records 890000 to 895000...\n",
      "Fetching records 895000 to 900000...\n",
      "Fetching records 900000 to 905000...\n",
      "Fetching records 905000 to 910000...\n",
      "Fetching records 910000 to 915000...\n",
      "Fetching records 915000 to 920000...\n",
      "Fetching records 920000 to 925000...\n",
      "Fetching records 925000 to 930000...\n",
      "Fetching records 930000 to 935000...\n",
      "Fetching records 935000 to 940000...\n",
      "Fetching records 940000 to 945000...\n",
      "Fetching records 945000 to 950000...\n",
      "Fetching records 950000 to 955000...\n",
      "Fetching records 955000 to 960000...\n",
      "Fetching records 960000 to 965000...\n",
      "Fetching records 965000 to 970000...\n",
      "Fetching records 970000 to 975000...\n",
      "Fetching records 975000 to 980000...\n",
      "Fetching records 980000 to 985000...\n",
      "Fetching records 985000 to 990000...\n",
      "Fetching records 990000 to 995000...\n",
      "Fetching records 995000 to 1000000...\n",
      "Fetching records 1000000 to 1005000...\n",
      "Fetching records 1005000 to 1010000...\n",
      "Fetching records 1010000 to 1015000...\n",
      "Fetching records 1015000 to 1020000...\n",
      "Fetching records 1020000 to 1025000...\n",
      "Fetching records 1025000 to 1030000...\n",
      "Fetching records 1030000 to 1035000...\n",
      "Fetching records 1035000 to 1040000...\n",
      "Fetching records 1040000 to 1045000...\n",
      "Fetching records 1045000 to 1050000...\n",
      "Fetching records 1050000 to 1055000...\n",
      "Fetching records 1055000 to 1060000...\n",
      "Fetching records 1060000 to 1065000...\n",
      "Fetching records 1065000 to 1070000...\n",
      "Fetching records 1070000 to 1075000...\n",
      "Fetching records 1075000 to 1080000...\n",
      "Fetching records 1080000 to 1085000...\n",
      "Fetching records 1085000 to 1090000...\n",
      "Fetching records 1090000 to 1095000...\n",
      "Fetching records 1095000 to 1100000...\n",
      "Fetching records 1100000 to 1105000...\n",
      "Fetching records 1105000 to 1110000...\n",
      "Fetching records 1110000 to 1115000...\n",
      "Fetching records 1115000 to 1120000...\n",
      "Fetching records 1120000 to 1125000...\n",
      "Fetching records 1125000 to 1130000...\n",
      "Fetching records 1130000 to 1135000...\n",
      "Fetching records 1135000 to 1140000...\n",
      "Fetching records 1140000 to 1145000...\n",
      "Fetching records 1145000 to 1150000...\n",
      "Fetching records 1150000 to 1155000...\n",
      "Fetching records 1155000 to 1160000...\n",
      "Fetching records 1160000 to 1165000...\n",
      "Fetching records 1165000 to 1170000...\n",
      "Fetching records 1170000 to 1175000...\n",
      "Fetching records 1175000 to 1180000...\n",
      "Fetching records 1180000 to 1185000...\n",
      "Fetching records 1185000 to 1190000...\n",
      "Fetching records 1190000 to 1195000...\n",
      "Fetching records 1195000 to 1200000...\n",
      "Fetching records 1200000 to 1205000...\n",
      "Fetching records 1205000 to 1210000...\n",
      "Fetching records 1210000 to 1215000...\n",
      "Fetching records 1215000 to 1220000...\n",
      "Fetching records 1220000 to 1225000...\n",
      "Fetching records 1225000 to 1230000...\n",
      "Fetching records 1230000 to 1235000...\n",
      "Fetching records 1235000 to 1240000...\n",
      "Fetching records 1240000 to 1245000...\n",
      "Fetching records 1245000 to 1250000...\n",
      "Fetching records 1250000 to 1255000...\n",
      "Fetching records 1255000 to 1260000...\n",
      "Fetching records 1260000 to 1265000...\n",
      "Fetching records 1265000 to 1270000...\n",
      "Fetching records 1270000 to 1275000...\n",
      "Fetching records 1275000 to 1280000...\n",
      "Fetching records 1280000 to 1285000...\n",
      "Fetching records 1285000 to 1290000...\n",
      "Fetching records 1290000 to 1295000...\n",
      "Fetching records 1295000 to 1300000...\n",
      "Fetching records 1300000 to 1305000...\n",
      "Fetching records 1305000 to 1310000...\n",
      "Fetching records 1310000 to 1315000...\n",
      "Fetching records 1315000 to 1320000...\n",
      "Fetching records 1320000 to 1325000...\n",
      "Fetching records 1325000 to 1330000...\n",
      "Fetching records 1330000 to 1335000...\n",
      "Fetching records 1335000 to 1340000...\n",
      "Fetching records 1340000 to 1345000...\n",
      "Fetching records 1345000 to 1350000...\n",
      "Fetching records 1350000 to 1355000...\n",
      "Fetching records 1355000 to 1360000...\n",
      "Fetching records 1360000 to 1365000...\n",
      "Fetching records 1365000 to 1370000...\n",
      "Fetching records 1370000 to 1375000...\n",
      "Fetching records 1375000 to 1380000...\n",
      "Fetching records 1380000 to 1385000...\n",
      "Fetching records 1385000 to 1390000...\n",
      "Fetching records 1390000 to 1395000...\n",
      "Fetching records 1395000 to 1400000...\n",
      "Fetching records 1400000 to 1405000...\n",
      "Fetching records 1405000 to 1410000...\n",
      "Fetching records 1410000 to 1415000...\n",
      "Fetching records 1415000 to 1420000...\n",
      "Fetching records 1420000 to 1425000...\n",
      "Fetching records 1425000 to 1430000...\n",
      "✅ Total records fetched: 1425802\n",
      "📄 Flattened CSV saved to /tmp/eia_data_energy_source_flat.csv\n",
      "🧾 Flattened columns: ['period', 'respondent', 'respondent-name', 'fueltype', 'type-name', 'timezone', 'timezone-description', 'value', 'value-units']\n",
      "✅ Uploaded raw JSON to s3://ecogridaidata/eia_electricity/eia_data_energy_source_20250323_200432.json\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "api_key = \"g8IFZafpLwkh0hRmYC8xE91RQ1qa0Rggf567XdZe\"\n",
    "base_url = \"https://api.eia.gov/v2/electricity/rto/daily-fuel-type-data/data/\"\n",
    "bucket_name = \"ecogridaidata\"\n",
    "s3_folder = \"eia_electricity\"\n",
    "target_total = 1_425_802\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === STATIC PARAMS TEMPLATE (no offset/length here) ===\n",
    "params_template = {\n",
    "    \"api_key\": api_key,\n",
    "    \"frequency\": \"daily\",\n",
    "    \"start\": \"2023-01-01\",\n",
    "    \"end\": \"2024-12-31\",\n",
    "    \"data[0]\": \"value\",\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"desc\"\n",
    "}-\n",
    "\n",
    "# === DATA COLLECTION LOOP ===\n",
    "all_data = []\n",
    "offset = 0\n",
    "length = 5000\n",
    "\n",
    "while len(all_data) < target_total:\n",
    "    print(f\"Fetching records {offset} to {offset + length}...\")\n",
    "\n",
    "    # Combine static params with dynamic offset and length\n",
    "    params = params_template.copy()\n",
    "    params[\"offset\"] = offset\n",
    "    params[\"length\"] = length\n",
    "\n",
    "    response = requests.get(\n",
    "        base_url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error at offset {offset}. Status code: {response.status_code}\")\n",
    "        try:\n",
    "            print(\"❗ Error details:\", response.json())\n",
    "        except:\n",
    "            print(\"❗ Could not parse error response.\")\n",
    "        break\n",
    "\n",
    "    response_json = response.json()\n",
    "    records = response_json.get(\"response\", {}).get(\"data\", [])\n",
    "\n",
    "    if not records:\n",
    "        print(\"No more data returned by API.\")\n",
    "        break\n",
    "\n",
    "    # 👀 Inspect sample record on first batch\n",
    "    if offset == 0 and records:\n",
    "        print(\"🔍 Sample keys in record:\", records[0].keys())\n",
    "        print(\"📦 Sample record preview:\\n\", json.dumps(records[0], indent=2))\n",
    "\n",
    "    all_data.extend(records)\n",
    "    offset += length\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Trim if slightly over\n",
    "    if len(all_data) > target_total:\n",
    "        all_data = all_data[:target_total]\n",
    "        print(\"Trimmed to exact target record count.\")\n",
    "\n",
    "print(f\"✅ Total records fetched: {len(all_data)}\")\n",
    "\n",
    "# === SAVE RAW JSON ===\n",
    "json_path = \"/tmp/eia_data_energy_source.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "\n",
    "# === FLATTEN AND SAVE PREVIEW CSV ===\n",
    "df = json_normalize(all_data)\n",
    "df.columns = [col.replace(\".\", \"_\") for col in df.columns]\n",
    "\n",
    "csv_path = \"/tmp/eia_data_energy_source_flat.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"📄 Flattened CSV saved to {csv_path}\")\n",
    "print(\"🧾 Flattened columns:\", df.columns.tolist())\n",
    "\n",
    "# === UPLOAD TO S3 (JSON file) ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "s3_key = f\"{s3_folder}/eia_data_energy_source_{timestamp}.json\"\n",
    "\n",
    "s3.upload_file(json_path, bucket_name, s3_key)\n",
    "print(f\"✅ Uploaded raw JSON to s3://{bucket_name}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "api_key = \"g8IFZafpLwkh0hRmYC8xE91RQ1qa0Rggf567XdZe\"\n",
    "base_url = \"https://api.eia.gov/v2/electricity/rto/daily-interchange-data/data/\"\n",
    "bucket_name = \"ecogridaidata\"\n",
    "s3_folder = \"eia_electricity\"\n",
    "target_total = 1_233_232\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === STATIC PARAMS TEMPLATE ===\n",
    "params_template = {\n",
    "    \"api_key\": api_key,\n",
    "    \"frequency\": \"daily\",\n",
    "    \"start\": \"2023-01-01\",\n",
    "    \"end\": \"2024-12-31\",\n",
    "    \"data[0]\": \"value\",\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"desc\"\n",
    "}\n",
    "\n",
    "# === DATA COLLECTION LOOP ===\n",
    "all_data = []\n",
    "offset = 0\n",
    "length = 5000\n",
    "\n",
    "while len(all_data) < target_total:\n",
    "    print(f\"Fetching records {offset} to {offset + length}...\")\n",
    "\n",
    "    # Merge template with dynamic offset/length\n",
    "    params = params_template.copy()\n",
    "    params[\"offset\"] = offset\n",
    "    params[\"length\"] = length\n",
    "\n",
    "    response = requests.get(\n",
    "        base_url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"❌ Error at offset {offset}. Status code: {response.status_code}\")\n",
    "        try:\n",
    "            print(\"Details:\", response.json())\n",
    "        except:\n",
    "            print(\"Could not parse error response.\")\n",
    "        break\n",
    "\n",
    "    response_json = response.json()\n",
    "    records = response_json.get(\"response\", {}).get(\"data\", [])\n",
    "\n",
    "    if not records:\n",
    "        print(\"No more data returned by API.\")\n",
    "        break\n",
    "\n",
    "    if offset == 0 and records:\n",
    "        print(\"🔍 Sample keys in record:\", records[0].keys())\n",
    "        print(\"📦 Sample record preview:\\n\", json.dumps(records[0], indent=2))\n",
    "\n",
    "    all_data.extend(records)\n",
    "    offset += length\n",
    "    time.sleep(1)\n",
    "\n",
    "    if len(all_data) > target_total:\n",
    "        all_data = all_data[:target_total]\n",
    "        print(\"Trimmed to exact target record count.\")\n",
    "\n",
    "print(f\"✅ Total records fetched: {len(all_data)}\")\n",
    "\n",
    "# === SAVE RAW JSON ===\n",
    "json_path = \"/tmp/eia_data_neighboring_bal.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "\n",
    "# === FLATTEN AND SAVE PREVIEW CSV ===\n",
    "df = json_normalize(all_data)\n",
    "df.columns = [col.replace(\".\", \"_\") for col in df.columns]\n",
    "\n",
    "csv_path = \"/tmp/eia_data_neighboring_bal_flat.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"📄 Flattened CSV saved to {csv_path}\")\n",
    "print(\"🧾 Flattened columns:\", df.columns.tolist())\n",
    "\n",
    "# === UPLOAD TO S3 ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "s3_key = f\"{s3_folder}/eia_data_neighboring_bal_{timestamp}.json\"\n",
    "\n",
    "s3.upload_file(json_path, bucket_name, s3_key)\n",
    "print(f\"✅ Uploaded raw JSON to s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "        \n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "    Jupyter.notebook.save_checkpoint();\n",
       "    Jupyter.notebook.session.delete();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
